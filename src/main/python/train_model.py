import tensorflow as tf

from lib import read_training_data, training_file, build_dataset, split_input_target, to_string, model_file, plot_graph

physical_devices = tf.config.experimental.list_physical_devices('GPU')

# Configuration
num_epochs = 300
batch_size = 20
num_shuffles = 100
# LSTM layers, the numbers are number of output, to the next layer but also to the next same type of layer (ordered).
n_hidden = [512, 256, 128]
# number of words sequence to predict the following words
n_input = 10
# number of words generated by the RNN
n_output = 10


def build_model():
    # Parameters
    # number of units in RNN cells
    rnn_model = tf.keras.Sequential()
    rnn_model.add(tf.keras.layers.Embedding(vocabulary_size, 128, input_length=n_input))  # fully connected layer
    for n_h in n_hidden:
        rnn_model.add(tf.keras.layers.LSTM(n_h, return_sequences=True, name='lstm%d' % n_h))
    # model.add(tf.keras.layers.Dropout(0.2)) # dropout to prevent overfitting

    rnn_model.add(tf.keras.layers.Dense(vocabulary_size, activation='softmax'))

    # Compile model
    rnn_model.compile(optimizer='RMSProp',  # or 'adam'
                      loss='sparse_categorical_crossentropy',
                      metrics=['accuracy'])

    rnn_model.summary()
    weights = rnn_model.get_weights()
    return rnn_model, weights


training_data = read_training_data(training_file)

word_to_int_map, int_to_word_map = build_dataset(training_data)
vocabulary_size = len(word_to_int_map)
print(word_to_int_map)
words_as_int = [word_to_int_map[w] for w in training_data]
print(words_as_int)

model, initial_weights = build_model()

# create tf.data.Dataset object, data encoded as integers
word_dataset = tf.data.Dataset.from_tensor_slices(words_as_int)

# take method generates elements (to verify):
for i in word_dataset.take(5):
    print(int_to_word_map[i.numpy()])

# The `batch` method creates dataset, that generates sequences of elements:
# obtain batches of size number of input +1, therefore these words are linked so then we can shuffle them after
sequences = word_dataset.batch(n_input + 1,
                               drop_remainder=True)

for item in sequences.take(5):
    print(to_string(item.numpy(), int_to_word_map))

dataset = sequences.map(split_input_target)

# Finally we shuffle the items, and produce minibatches of 16 elements:
# 16 batches to save resources dataset
dataset = dataset.shuffle(num_shuffles).batch(batch_size, drop_remainder=True)

save_checkpoint = tf.keras.callbacks.ModelCheckpoint('checkpoints/checkpoint.{epoch:02d}.h5',
                                                     save_weights_only=False, save_best_only=False, save_freq=100000,
                                                     verbose=1)
# train model
model.set_weights(initial_weights)
history = model.fit(dataset, callbacks=[save_checkpoint], epochs=num_epochs, verbose=1)
# save model
model.save(model_file + '.h5')
plot_graph(history)
